{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 设置环境变量\n",
    "os.environ['HF_HOME'] = './cache'\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 77, 768])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class Embed(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Embed, self).__init__()   #在python3中使用super().__init__()即可\n",
    "\n",
    "        self.embed = torch.nn.Embedding(49408, 768) # 49408是词表大小，768是词向量维度\n",
    "        self.pos_embed = torch.nn.Embedding(77, 768)    # 77是最大句子长度\n",
    "\n",
    "        # 注册一个pos_ids张量，这个张量不需要梯度，不需要更新\n",
    "        self.register_buffer('pos_ids', torch.arange(77).unsqueeze(dim=0))\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # input_ids: [batch_size, seq_len] ~ [b, 77]\n",
    "        #这是模型的输入，代表一个批次中的数据，每个样本是长度为77的序列，序列中的每个元素是一个整数索引，代表一个单词。\n",
    "\n",
    "        # [b, 77] -> [b, 77, 768]\n",
    "        # self.embed是一个嵌入层，它将input_ids中的每个单词索引映射到一个768维的向量。\n",
    "        # 因此，对于输入中的每个单词，嵌入层返回一个向量，为整个批次生成一个[b, 77, 768]的张量。\n",
    "        embed = self.embed(input_ids)  # [batch_size, seq_len, 768]\n",
    "\n",
    "        # [1, 77] -> [1, 77, 768]\n",
    "        # self.pos_embed是另一个嵌入层，用于学习位置编码。self.pos_ids是一个预先注册的张量，其中包含从0到76的整数，表示句子中每个位置的索引。\n",
    "        # 位置嵌入层将这些位置索引映射到768维的向量。\n",
    "        # 因为pos_ids是一个[1, 77]的张量，所以pos_embeds的结果维度将是[1, 77, 768]。\n",
    "        pos_embeds = self.pos_embed(self.pos_ids)\n",
    "\n",
    "        # [b, 77, 768] + [1, 77, 768] -> [b, 77, 768]\n",
    "        # 最后，将单词嵌入embed和位置嵌入pos_embeds相加。\n",
    "        # 由于pos_embeds的第一个维度是1，它会在加法操作中广播到输入的批次大小b。\n",
    "        # 这意味着位置嵌入将被复制到每个样本上，使得每个单词的嵌入都加上相应的位置嵌入。\n",
    "        # 最终，输出张量的维度仍然是[b, 77, 768]，其中每个单词的嵌入现在包含了关于它在序列中位置的信息。\n",
    "        return embed + pos_embeds\n",
    "\n",
    "Embed()(torch.rand(4, 77).long()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 77, 768])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Atten(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Atten, self).__init__()\n",
    "\n",
    "        self.q = torch.nn.Linear(768,768)\n",
    "        self.k = torch.nn.Linear(768,768)\n",
    "        self.v = torch.nn.Linear(768,768)\n",
    "        self.out = torch.nn.Linear(768,768)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [b, 77, 768]\n",
    "        b = x.shape[0]\n",
    "\n",
    "        #维度不变\n",
    "        #[b, 77, 768]\n",
    "        q = self.q(x) * 0.125   #当它们被分成12个头时，每个头的维度变为64（因为 768 / 12 = 64）。因此，理论上应该是除以 sqrt(64)，等同于除以8。\n",
    "        k = self.k(x)\n",
    "        v = self.v(x)\n",
    "\n",
    "        #拆分注意力头\n",
    "        # [b, 77, 768] -> [b, 77, 12, 64] -> [b, 12, 77, 64] -> [b*12, 77, 64]\n",
    "        q = q.reshape(b, 77, 12, 64).transpose(1, 2).reshape(b*12, 77, 64)\n",
    "        k = k.reshape(b, 77, 12, 64).transpose(1, 2).reshape(b*12, 77, 64)\n",
    "        v = v.reshape(b, 77, 12, 64).transpose(1, 2).reshape(b*12, 77, 64)\n",
    "\n",
    "        #注意力得分\n",
    "        # [b*12, 77, 64] @ [b*12, 64, 77] -> [b*12, 77, 77], @是矩阵乘法\n",
    "        attn = torch.bmm(q, k.transpose(1, 2))\n",
    "\n",
    "        # 拆分注意力头\n",
    "        # [b*12, 77, 77] -> [b, 12, 77, 77]\n",
    "        attn = attn.reshape(b, 12, 77, 77)\n",
    "\n",
    "        #覆盖mask\n",
    "        def get_mask(b):\n",
    "            mask = torch.empty(b, 77, 77)\n",
    "\n",
    "            #上三角的部分置为负无穷\n",
    "            mask.fill_(float('-inf'))\n",
    "\n",
    "            #对角线及其下三角部分置为0\n",
    "            mask.triu_(diagonal=1)\n",
    "\n",
    "            return mask.unsqueeze(1)    #unsqueeze(1)是为了在第二维上增加一个维度,变成[b, 1, 77, 77]\n",
    "\n",
    "        # [b, 12, 77, 77] + [b, 1, 77, 77] -> [b, 12, 77, 77]\n",
    "        attn = attn + get_mask(attn.shape[0]).to(attn.device)\n",
    "\n",
    "        # [b, 12, 77, 77] -> [b*12, 77, 77]\n",
    "        attn = attn.reshape(b * 12, 77, 77)\n",
    "\n",
    "        #计算softmax,被mask的部分值为0\n",
    "        attn = attn.softmax(dim=-1)\n",
    "\n",
    "        #计算和v的乘积\n",
    "        #[b*12, 77, 77] @ [b*12, 77, 64] -> [b*12, 77, 64]\n",
    "        attn = torch.bmm(attn, v)\n",
    "\n",
    "        #[b*12, 77, 64] -> [b, 12, 77, 64] -> [b, 77, 12, 64] -> [b, 77, 768]\n",
    "        attn = attn.reshape(b, 12, 77, 64).transpose(1, 2).reshape(b, 77, 768)\n",
    "\n",
    "        #线性输出，维度不变\n",
    "        #[b, 77, 768]\n",
    "        return self.out(attn)\n",
    "\n",
    "Atten()(torch.rand(2, 77, 768)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 77, 768])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ClipEncoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ClipEncoder, self).__init__()\n",
    "\n",
    "        self.s1 = torch.nn.Sequential(\n",
    "            torch.nn.LayerNorm(768),    #数据预处理阶段的归一化通常不涉及学习参数，而神经网络中的标准化技术则包含可学习的参数，这些参数是模型训练的一部分。\n",
    "            Atten(),\n",
    "        )\n",
    "\n",
    "        self.s2 = torch.nn.Sequential(\n",
    "            torch.nn.LayerNorm(768),\n",
    "            #如果模型的维度（通常表示为d_model）是768，那么FFN内部层的维度通常会被设置为4 * d_model，即3072。\n",
    "            # 这个维度的扩大可以给模型带来更多的表达能力，但同时也会增加模型的参数量和计算负担。\n",
    "            # 选择3072是基于经验的，你可以根据具体任务和计算资源自行调整这个大小。\n",
    "            torch.nn.Linear(768, 3072),\n",
    "        )\n",
    "\n",
    "        self.s3 = torch.nn.Linear(3072, 768)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x: [b, 77, 768]\n",
    "\n",
    "        #维度不变\n",
    "        #[b, 77, 768]\n",
    "        x = x + self.s1(x)\n",
    "\n",
    "        #[b, 77, 768]\n",
    "        res = x\n",
    "\n",
    "        #[b, 77, 768] -> [b, 77, 3072]\n",
    "        x = self.s2(x)\n",
    "\n",
    "        #维度不变\n",
    "        #[b, 77, 3072]\n",
    "        x = x * (x * 1.702).sigmoid()   #GELU（Gaussian Error Linear Unit）激活函数的变种。\n",
    "\n",
    "        #[b, 77, 3072] -> [b, 77, 768]\n",
    "        return res + self.s3(x)\n",
    "\n",
    "ClipEncoder()(torch.rand(2, 77, 768)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 77, 768])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = torch.nn.Sequential(\n",
    "    Embed(),\n",
    "    ClipEncoder(),\n",
    "    ClipEncoder(),\n",
    "    ClipEncoder(),\n",
    "    ClipEncoder(),\n",
    "    ClipEncoder(),\n",
    "    ClipEncoder(),\n",
    "    ClipEncoder(),\n",
    "    ClipEncoder(),\n",
    "    ClipEncoder(),\n",
    "    ClipEncoder(),\n",
    "    ClipEncoder(),\n",
    "    ClipEncoder(),\n",
    "    torch.nn.LayerNorm(768),\n",
    ")\n",
    "\n",
    "encoder(torch.rand(2, 77).long()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Embed(\n",
      "    (embed): Embedding(49408, 768)\n",
      "    (pos_embed): Embedding(77, 768)\n",
      "  )\n",
      "  (1): ClipEncoder(\n",
      "    (s1): Sequential(\n",
      "      (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Atten(\n",
      "        (q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (s2): Sequential(\n",
      "      (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    )\n",
      "    (s3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  )\n",
      "  (2): ClipEncoder(\n",
      "    (s1): Sequential(\n",
      "      (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Atten(\n",
      "        (q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (s2): Sequential(\n",
      "      (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    )\n",
      "    (s3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  )\n",
      "  (3): ClipEncoder(\n",
      "    (s1): Sequential(\n",
      "      (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Atten(\n",
      "        (q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (s2): Sequential(\n",
      "      (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    )\n",
      "    (s3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  )\n",
      "  (4): ClipEncoder(\n",
      "    (s1): Sequential(\n",
      "      (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Atten(\n",
      "        (q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (s2): Sequential(\n",
      "      (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    )\n",
      "    (s3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  )\n",
      "  (5): ClipEncoder(\n",
      "    (s1): Sequential(\n",
      "      (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Atten(\n",
      "        (q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (s2): Sequential(\n",
      "      (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    )\n",
      "    (s3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  )\n",
      "  (6): ClipEncoder(\n",
      "    (s1): Sequential(\n",
      "      (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Atten(\n",
      "        (q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (s2): Sequential(\n",
      "      (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    )\n",
      "    (s3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  )\n",
      "  (7): ClipEncoder(\n",
      "    (s1): Sequential(\n",
      "      (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Atten(\n",
      "        (q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (s2): Sequential(\n",
      "      (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    )\n",
      "    (s3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  )\n",
      "  (8): ClipEncoder(\n",
      "    (s1): Sequential(\n",
      "      (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Atten(\n",
      "        (q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (s2): Sequential(\n",
      "      (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    )\n",
      "    (s3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  )\n",
      "  (9): ClipEncoder(\n",
      "    (s1): Sequential(\n",
      "      (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Atten(\n",
      "        (q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (s2): Sequential(\n",
      "      (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    )\n",
      "    (s3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  )\n",
      "  (10): ClipEncoder(\n",
      "    (s1): Sequential(\n",
      "      (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Atten(\n",
      "        (q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (s2): Sequential(\n",
      "      (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    )\n",
      "    (s3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  )\n",
      "  (11): ClipEncoder(\n",
      "    (s1): Sequential(\n",
      "      (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Atten(\n",
      "        (q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (s2): Sequential(\n",
      "      (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    )\n",
      "    (s3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  )\n",
      "  (12): ClipEncoder(\n",
      "    (s1): Sequential(\n",
      "      (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Atten(\n",
      "        (q): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (k): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (v): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (s2): Sequential(\n",
      "      (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    )\n",
      "    (s3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "  )\n",
      "  (13): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import CLIPTextModel\n",
    "\n",
    "# 加载预训练模型的参数\n",
    "params = CLIPTextModel.from_pretrained(\n",
    "    \"./Diffusion_model\", subfolder=\"text_encoder\"\n",
    ")\n",
    "\n",
    "#词编码\n",
    "encoder[0].embed.load_state_dict(params.text_model.embeddings.token_embedding.state_dict())\n",
    "\n",
    "#位置编码\n",
    "encoder[0].pos_embed.load_state_dict(params.text_model.embeddings.position_embedding.state_dict())\n",
    "\n",
    "#12层编码层\n",
    "for i in range(12):\n",
    "\n",
    "    #第一层norm\n",
    "    encoder[i + 1].s1[0].load_state_dict(params.text_model.encoder.layers[i].layer_norm1.state_dict())\n",
    "\n",
    "    #注意力q矩阵\n",
    "    encoder[i + 1].s1[1].q.load_state_dict(params.text_model.encoder.layers[i].self_attn.q_proj.state_dict())\n",
    "\n",
    "    #注意力k矩阵\n",
    "    encoder[i + 1].s1[1].k.load_state_dict(params.text_model.encoder.layers[i].self_attn.k_proj.state_dict())\n",
    "\n",
    "    #注意力v矩阵\n",
    "    encoder[i + 1].s1[1].v.load_state_dict(params.text_model.encoder.layers[i].self_attn.v_proj.state_dict())\n",
    "\n",
    "    #注意力out\n",
    "    encoder[i + 1].s1[1].out.load_state_dict(params.text_model.encoder.layers[i].self_attn.out_proj.state_dict())\n",
    "\n",
    "    #第二层norm\n",
    "    encoder[i + 1].s2[0].load_state_dict(params.text_model.encoder.layers[i].layer_norm2.state_dict())\n",
    "\n",
    "    #mlp第一层fc\n",
    "    encoder[i + 1].s2[1].load_state_dict(params.text_model.encoder.layers[i].mlp.fc1.state_dict())\n",
    "\n",
    "    #mlp第二层fc\n",
    "    encoder[i + 1].s3.load_state_dict(params.text_model.encoder.layers[i].mlp.fc2.state_dict())\n",
    "\n",
    "encoder[13].load_state_dict(params.text_model.final_layer_norm.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#检查模型构建正确无误\n",
    "a = encoder(torch.arange(77).unsqueeze(0))\n",
    "b = params(torch.arange(77).unsqueeze(0)).last_hidden_state\n",
    "\n",
    "(a == b).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 10, 10])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Resnet(torch.nn.Module):\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super().__init__()\n",
    "\n",
    "        self.s = torch.nn.Sequential(\n",
    "            torch.nn.GroupNorm(\n",
    "                num_groups=32, num_channels=dim_in, eps=1e-6, affine=True\n",
    "            ),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Conv2d(dim_in, dim_out, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.GroupNorm(\n",
    "                num_groups=32, num_channels=dim_out, eps=1e-6, affine=True\n",
    "            ),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Conv2d(dim_out, dim_out, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "\n",
    "        self.res = None\n",
    "        if dim_in != dim_out:\n",
    "            self.res = torch.nn.Conv2d(dim_in, dim_out, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x: [1, 128, 10, 10]\n",
    "\n",
    "        res = x\n",
    "        if self.res:\n",
    "            #[1, 128, 10, 10] -> [1, 256, 10, 10]\n",
    "            res = self.res(x)\n",
    "\n",
    "        #[1, 128, 10, 10] -> [1, 256, 10, 10]\n",
    "        return res + self.s(x)\n",
    "\n",
    "Resnet(128, 256)(torch.randn(1, 128, 10, 10)).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 64, 64])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Atten(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.norm = torch.nn.GroupNorm(\n",
    "            num_channels=512, num_groups=32, eps=1e-6, affine=True\n",
    "        )\n",
    "\n",
    "        self.q = torch.nn.Linear(512, 512)\n",
    "        self.k = torch.nn.Linear(512, 512)\n",
    "        self.v = torch.nn.Linear(512, 512)\n",
    "        self.out = torch.nn.Linear(512, 512)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [1, 512, 64, 64]\n",
    "        res = x\n",
    "\n",
    "        # norm,维度不变\n",
    "        # [1, 512, 64, 64]\n",
    "        x = self.norm(x)\n",
    "\n",
    "        # [1, 512, 64, 64] -> [1, 512, 4096] -> [1, 4096, 512]\n",
    "        x = x.flatten(2).transpose(1, 2)  # flatten(2)是将后两维展平，transpose(1, 2)是交换第二维和第三维\n",
    "\n",
    "        # 线性运算，维度不变\n",
    "        # [1, 4096, 512]\n",
    "        q = self.q(x)\n",
    "        k = self.k(x)\n",
    "        v = self.v(x)\n",
    "\n",
    "        # [1, 4096, 512] -> [1, 512, 4096]\n",
    "        k = k.transpose(1, 2)\n",
    "\n",
    "        # [1, 4096, 512] * [1, 512, 4096] -> [1, 4096, 4096]\n",
    "        atten = torch.baddbmm(\n",
    "            torch.empty(1, 4096, 4096, device=q.device), q, k, beta=0, alpha=(512**-0.5)\n",
    "        )\n",
    "\n",
    "        atten = torch.softmax(atten, dim=2)\n",
    "\n",
    "        #[1, 4096, 4096] * [1, 4096, 512] -> [1, 4096, 512]\n",
    "        atten = atten.bmm(v)\n",
    "\n",
    "        #线性运算，维度不变\n",
    "        # [1, 4096, 512]\n",
    "        atten = self.out(atten)\n",
    "\n",
    "        # [1, 4096, 512] -> [1, 512, 4096] -> [1, 512, 64, 64]\n",
    "        atten = atten.transpose(1, 2).view(1, 512, 64, 64)\n",
    "\n",
    "        #残差连接，维度不变\n",
    "        # [1, 512, 64, 64]\n",
    "        atten = atten + res\n",
    "\n",
    "        return atten\n",
    "\n",
    "Atten()(torch.randn(1, 512, 64, 64)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1., 1., 1., 1., 1., 0.],\n",
       "          [1., 1., 1., 1., 1., 0.],\n",
       "          [1., 1., 1., 1., 1., 0.],\n",
       "          [1., 1., 1., 1., 1., 0.],\n",
       "          [1., 1., 1., 1., 1., 0.],\n",
       "          [0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "         [[1., 1., 1., 1., 1., 0.],\n",
       "          [1., 1., 1., 1., 1., 0.],\n",
       "          [1., 1., 1., 1., 1., 0.],\n",
       "          [1., 1., 1., 1., 1., 0.],\n",
       "          [1., 1., 1., 1., 1., 0.],\n",
       "          [0., 0., 0., 0., 0., 0.]]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Pad(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        # x: [1, 512, 64, 64]\n",
    "        return torch.nn.functional.pad(x, (0, 1, 0, 1), mode=\"constant\", value=0)   #pad函数的参数是左右上下的填充数,modes是填充的方式，value是填充的值\n",
    "\n",
    "Pad()(torch.ones(1, 2, 5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 512, 512])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VAE(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            #in\n",
    "            torch.nn.Conv2d(3, 128, kernel_size=3, stride=1, padding=1),\n",
    "\n",
    "            #down\n",
    "            torch.nn.Sequential(\n",
    "                Resnet(128, 128),\n",
    "                Resnet(128, 128),\n",
    "                torch.nn.Sequential(\n",
    "                    Pad(),\n",
    "                    torch.nn.Conv2d(128, 128, 3, stride=2, padding=0),\n",
    "                ),\n",
    "            ),\n",
    "            torch.nn.Sequential(\n",
    "                Resnet(128, 256),\n",
    "                Resnet(256, 256),\n",
    "                torch.nn.Sequential(\n",
    "                    Pad(),\n",
    "                    torch.nn.Conv2d(256, 256, 3, stride=2, padding=0),\n",
    "                ),\n",
    "            ),\n",
    "            torch.nn.Sequential(\n",
    "                Resnet(256, 512),\n",
    "                Resnet(512, 512),\n",
    "                torch.nn.Sequential(\n",
    "                    Pad(),\n",
    "                    torch.nn.Conv2d(512, 512, 3, stride=2, padding=0),\n",
    "                ),\n",
    "            ),\n",
    "            torch.nn.Sequential(\n",
    "                Resnet(512, 512),\n",
    "                Resnet(512, 512),\n",
    "            ),\n",
    "\n",
    "            #mid\n",
    "            torch.nn.Sequential(\n",
    "                Resnet(512, 512),\n",
    "                Atten(),\n",
    "                Resnet(512, 512),\n",
    "            ),\n",
    "\n",
    "            #out\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.GroupNorm(num_channels=512, num_groups=32, eps=1e-6),\n",
    "                torch.nn.SiLU(),\n",
    "                torch.nn.Conv2d(512, 8, 3, padding=1),\n",
    "            ),\n",
    "\n",
    "            #正态分布层\n",
    "            torch.nn.Conv2d(8, 8, 1),\n",
    "        )\n",
    "\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            #正态分布层\n",
    "            torch.nn.Conv2d(4, 4, 1),\n",
    "\n",
    "            #in\n",
    "            torch.nn.Conv2d(4, 512, kernel_size=3, stride=1, padding=1),\n",
    "\n",
    "            #middle\n",
    "            torch.nn.Sequential(Resnet(512, 512), Atten(), Resnet(512, 512)),\n",
    "\n",
    "            #up\n",
    "            torch.nn.Sequential(\n",
    "                Resnet(512, 512),\n",
    "                Resnet(512, 512),\n",
    "                Resnet(512, 512),\n",
    "                torch.nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
    "                torch.nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            ),\n",
    "            torch.nn.Sequential(\n",
    "                Resnet(512, 512),\n",
    "                Resnet(512, 512),\n",
    "                Resnet(512, 512),\n",
    "                torch.nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
    "                torch.nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            ),\n",
    "            torch.nn.Sequential(\n",
    "                Resnet(512, 256),\n",
    "                Resnet(256, 256),\n",
    "                Resnet(256, 256),\n",
    "                torch.nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
    "                torch.nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            ),\n",
    "            torch.nn.Sequential(\n",
    "                Resnet(256, 128),\n",
    "                Resnet(128, 128),\n",
    "                Resnet(128, 128),\n",
    "            ),\n",
    "\n",
    "            #out\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.GroupNorm(num_channels=128, num_groups=32, eps=1e-6),\n",
    "                torch.nn.SiLU(),\n",
    "                torch.nn.Conv2d(128, 3, 3, padding=1),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def sample(self, h):\n",
    "        #h: [1, 8, 64, 64]\n",
    "\n",
    "        #[1, 4, 64, 64]\n",
    "        mean = h[:, :4]\n",
    "        logvar = h[:, 4:]\n",
    "        std = logvar.exp()**0.5\n",
    "\n",
    "        #[1, 4, 64, 64]\n",
    "        h = torch.randn(mean.shape, device=mean.device)\n",
    "        h = mean + std * h\n",
    "\n",
    "        return h\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x: [1, 3, 512, 512]\n",
    "\n",
    "        #[1, 3, 512, 512] -> [1, 8, 64, 64]\n",
    "        h = self.encoder(x)\n",
    "\n",
    "        #[1, 8, 64, 64] -> [1, 4, 64, 64]\n",
    "        h = self.sample(h)\n",
    "\n",
    "        #[1, 4, 64, 64] -> [1, 3, 512, 512]\n",
    "        h = self.decoder(h)\n",
    "\n",
    "        return h\n",
    "\n",
    "VAE()(torch.randn(1, 3, 512, 512)).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 1.13.1+cu117 with CUDA 1107 (you have 1.13.1+cpu)\n",
      "    Python  3.9.13 (you have 3.9.18)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from diffusers import AutoencoderKL\n",
    "\n",
    "#加载预训练模型的参数\n",
    "params = AutoencoderKL.from_pretrained(\"./Diffusion_model\", subfolder=\"vae\")\n",
    "\n",
    "vae = VAE()\n",
    "\n",
    "def load_res(model, param):\n",
    "    model.s[0].load_state_dict(param.norm1.state_dict())\n",
    "    model.s[2].load_state_dict(param.conv1.state_dict())\n",
    "    model.s[3].load_state_dict(param.norm2.state_dict())\n",
    "    model.s[5].load_state_dict(param.conv2.state_dict())\n",
    "\n",
    "    if isinstance(model.res, torch.nn.Module):\n",
    "        model.res.load_state_dict(param.conv_shortcut.state_dict())\n",
    "\n",
    "def load_attn(model, param):\n",
    "    model.norm.load_state_dict(param.group_norm.state_dict())\n",
    "    model.q.load_state_dict(param.to_q.state_dict())\n",
    "    model.k.load_state_dict(param.to_k.state_dict())\n",
    "    model.v.load_state_dict(param.to_v.state_dict())\n",
    "    model.out.load_state_dict(param.to_out[0].state_dict())\n",
    "\n",
    "#encoder.in\n",
    "vae.encoder[0].load_state_dict(params.encoder.conv_in.state_dict())\n",
    "\n",
    "#encoder.down\n",
    "for i in range(4):\n",
    "    load_res(vae.encoder[i + 1][0], params.encoder.down_blocks[i].resnets[0])\n",
    "    load_res(vae.encoder[i + 1][1], params.encoder.down_blocks[i].resnets[1])\n",
    "\n",
    "    if i != 3:\n",
    "        vae.encoder[i + 1][2][1].load_state_dict(params.encoder.down_blocks[i].downsamplers[0].conv.state_dict())\n",
    "\n",
    "#encoder.mid\n",
    "load_res(vae.encoder[5][0], params.encoder.mid_block.resnets[0])\n",
    "load_res(vae.encoder[5][2], params.encoder.mid_block.resnets[1])\n",
    "load_attn(vae.encoder[5][1], params.encoder.mid_block.attentions[0])\n",
    "\n",
    "#encoder.out\n",
    "vae.encoder[6][0].load_state_dict(params.encoder.conv_norm_out.state_dict())\n",
    "vae.encoder[6][2].load_state_dict(params.encoder.conv_out.state_dict())\n",
    "\n",
    "#encoder.正态分布层\n",
    "vae.encoder[7].load_state_dict(params.quant_conv.state_dict())\n",
    "\n",
    "#decoder.正态分布层\n",
    "vae.decoder[0].load_state_dict(params.post_quant_conv.state_dict())\n",
    "\n",
    "#decoder.in\n",
    "vae.decoder[1].load_state_dict(params.decoder.conv_in.state_dict())\n",
    "\n",
    "#decoder.mid\n",
    "load_res(vae.decoder[2][0], params.decoder.mid_block.resnets[0])\n",
    "load_res(vae.decoder[2][2], params.decoder.mid_block.resnets[1])\n",
    "load_attn(vae.decoder[2][1], params.decoder.mid_block.attentions[0])\n",
    "\n",
    "#decoder.up\n",
    "for i in range(4):\n",
    "    load_res(vae.decoder[i + 3][0], params.decoder.up_blocks[i].resnets[0])\n",
    "    load_res(vae.decoder[i + 3][1], params.decoder.up_blocks[i].resnets[1])\n",
    "    load_res(vae.decoder[i + 3][2], params.decoder.up_blocks[i].resnets[2])\n",
    "\n",
    "    if i != 3:\n",
    "        vae.decoder[i + 3][4].load_state_dict(\n",
    "            params.decoder.up_blocks[i].upsamplers[0].conv.state_dict())\n",
    "\n",
    "#decoder.out\n",
    "vae.decoder[7][0].load_state_dict(params.decoder.conv_norm_out.state_dict())\n",
    "vae.decoder[7][2].load_state_dict(params.decoder.conv_out.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.randn(1, 3, 512, 512)\n",
    "\n",
    "a = vae.encoder(data)\n",
    "b = params.encode(data).latent_dist.parameters\n",
    "\n",
    "(a == b).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.randn(1, 4, 64, 64)\n",
    "\n",
    "a = vae.decoder(data)\n",
    "b = params.decode(data).sample\n",
    "\n",
    "(a == b).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 640, 64, 64])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Resnet(torch.nn.Module):\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super().__init__()\n",
    "\n",
    "        self.time = torch.nn.Sequential(\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(1280, dim_out),\n",
    "            torch.nn.Unflatten(dim=1, unflattened_size=(dim_out, 1, 1)),    #Unflatten是将张量展平后再恢复成原来的形状,dim是要展平的维度，unflattened_size指定了展开后的大小\n",
    "        )\n",
    "\n",
    "        self.s0 = torch.nn.Sequential(\n",
    "            torch.nn.GroupNorm(num_groups=32, num_channels=dim_in, eps=1e-05, affine=True),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Conv2d(dim_in, dim_out, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "\n",
    "        self.s1 = torch.nn.Sequential(\n",
    "            torch.nn.GroupNorm(num_groups=32, num_channels=dim_out, eps=1e-05, affine=True),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Conv2d(dim_out, dim_out, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "\n",
    "        self.res = None\n",
    "        if dim_in != dim_out:\n",
    "            self.res = torch.nn.Conv2d(dim_in, dim_out, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x, time):\n",
    "        #x: [1, 320, 64, 64]\n",
    "        #time: [1, 1280]\n",
    "\n",
    "        res = x\n",
    "\n",
    "        # [1, 1280] -> [1, 640, 1, 1]\n",
    "        time = self.time(time)\n",
    "\n",
    "        # [1, 320, 64, 64] -> [1, 640, 64, 64]\n",
    "        x = self.s0(x) + time\n",
    "\n",
    "        #维度不变\n",
    "        # [1, 640, 64, 64]\n",
    "        x = self.s1(x)\n",
    "\n",
    "        # [1, 320, 64, 64] -> [1, 640, 64, 64]\n",
    "        if self.res:\n",
    "            res = self.res(res)\n",
    "\n",
    "        #维度不变\n",
    "        # [1, 640, 32, 32]\n",
    "        x = res + x\n",
    "\n",
    "        return x\n",
    "\n",
    "Resnet(320, 640)(torch.randn(1, 320, 64, 64), torch.randn(1, 1280)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自注意力（Self-Attention）\n",
    "自注意力模型中，注意力权重是在同一个序列的不同位置之间计算的。换句话说，序列内部的元素相互计算注意力分数。\n",
    "它通常用于捕捉序列内的长距离依赖关系。例如，在Transformer模型中，自注意力允许模型在处理序列中的每个元素时考虑到序列中的其他元素。\n",
    "在自注意力中，查询（Query）、键（Key）和值（Value）通常来自同一个输入源。\n",
    "\n",
    "交叉注意力（Cross-Attention）\n",
    "交叉注意力通常涉及两个不同的序列或信息源。在这种情况下，一个序列的元素（查询）会和另一个序列的元素（键和值）计算注意力分数。\n",
    "这种类型的注意力机制常用于任务中需要对两个不同的输入进行交互和比较的情况，如在机器翻译中，源语言和目标语言之间的注意力，或者在图像文字处理任务中，图像特征和文字特征之间的注意力。\n",
    "在交叉注意力中，查询（Query）来自于一个输入源，而键（Key）和值（Value）来自于另一个输入源。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "操作 [1, 4096, 320] -> [8, 4096, 40] 将一个大的特征维度（在这里是320）分割成多个较小的特征空间（在这里是40）。每个头在自己的特征子空间中处理序列，然后最后的输出会被整合起来。这种分割方法如下：\n",
    "\n",
    "为什么分散：原始的Transformer模型引入了多头注意力机制来使模型能够在不同的表示子空间中捕捉到不同类型的信息。这是基于这样的假设：不同的注意力头可以学习到不同方面的特征。\n",
    "\n",
    "计算细节：在您的代码中，原始特征维度是320。当您分割成8个头时，每个头的特征维度变为 320 / 8 = 40。这意味着每个头可以关注输入序列特征的不同方面。\n",
    "\n",
    "效果：多头注意力通常可以提高模型的性能，因为它允许模型在不同的表示子空间中并行地捕捉到更加复杂和细粒度的信息。\n",
    "\n",
    "最后输出：最后，经过注意力计算后的多个头的输出会被重新组合（concatenated）或相加（summed），以形成单个输出张量，这个张量的特征维度通常会恢复到原始的维度大小。在您的代码中，这一步是通过 reshape(x) 函数的第二个定义完成的，将 [8, 4096, 40] 重新整合为 [1, 4096, 320]。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在深度学习中，mask_attention 或一般的注意力掩码（attention masking）被用于多种情况，主要是为了防止模型在计算注意力时考虑某些不应该被考虑的信息。以下是使用注意力掩码的几种常见场景：\n",
    "\n",
    "填充（Padding）处理：在处理不等长的序列数据时，通常会使用填充来使所有序列达到相同的长度。这样做可以让批处理变得可行。然而，在计算注意力时，我们不希望模型将填充的部分考虑进去，因为它们不包含有用的信息。因此，我们使用掩码来指示模型忽略这些填充位置。\n",
    "\n",
    "因果（Causal）或序列（Sequential）掩码：在生成文本或处理时间序列数据时，模型在预测位置 i 的输出时，只应该使用位置 i 之前的信息。为了确保模型不会\"看到未来\"，我们使用一个因果掩码，该掩码会覆盖序列中位置 i 之后的所有位置。\n",
    "\n",
    "解码器-编码器注意力掩码：在序列到序列模型（如Transformer模型）中，解码器的每一步都需要关注编码器的输出。如果编码器的输出包含了填充，我们需要确保解码器不会将这些填充考虑进注意力计算中。\n",
    "\n",
    "特定任务的掩码：有时，根据特定任务的需求，可能需要设计特殊的掩码。例如，在一些对齐或匹配任务中，可能只希望模型关注特定的输入对。\n",
    "\n",
    "在实际实现中，掩码通常是一个与输入序列形状相同的张量，掩码张量的元素通常由0和负无穷（或非常小的值）组成。在应用softmax函数之前，掩码张量会与注意力分数张量相加。由于softmax函数的性质，任何负无穷的元素都会导致对应位置的输出接近于0，从而有效地\"掩盖\"了这些位置的影响。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4096, 320])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CrossAttention(torch.nn.Module):\n",
    "    def __init__(self, dim_q, dim_kv):\n",
    "        #dim_q -> 320\n",
    "        #dim_kv -> 768\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim_q = dim_q\n",
    "        self.q = torch.nn.Linear(dim_q, dim_q, bias=False)\n",
    "        self.k = torch.nn.Linear(dim_kv, dim_q, bias=False)\n",
    "        self.v = torch.nn.Linear(dim_kv, dim_q, bias=False)\n",
    "\n",
    "        self.out = torch.nn.Linear(dim_q, dim_q)\n",
    "\n",
    "    def forward(self, q, kv):\n",
    "        # q: [1, 4096, 320]\n",
    "        # kv: [1, 77, 768]\n",
    "\n",
    "        # [1, 4096, 320] -> [1, 4096, 320]\n",
    "        q = self.q(q)\n",
    "        # [1, 77, 768] -> [1, 77, 320]\n",
    "        k = self.k(kv)\n",
    "        # [1, 77, 768] -> [1, 77, 320]\n",
    "        v = self.v(kv)\n",
    "\n",
    "        def reshape(x):\n",
    "            # x: [1, 4096, 320]\n",
    "            b, lens, dim = x.shape\n",
    "\n",
    "            # [1, 4096, 320] -> [1, 4096, 8, 40]\n",
    "            x = x.reshape(b, lens, 8, dim // 8)\n",
    "\n",
    "            # [1, 4096, 8, 40] -> [1, 8, 4096, 40]\n",
    "            x = x.transpose(1, 2)\n",
    "\n",
    "            # [1, 8, 4096, 40] -> [8, 4096, 40]\n",
    "            x = x.reshape(b * 8, lens, dim // 8)\n",
    "\n",
    "            return x\n",
    "\n",
    "        #[1, 4096, 320] -> [8, 4096, 40]    多头注意力，8头\n",
    "        q = reshape(q)\n",
    "        #[1, 77, 320] -> [8, 77, 40]\n",
    "        k = reshape(k)\n",
    "        #[1, 77, 320] -> [8, 77, 40]\n",
    "        v = reshape(v)\n",
    "\n",
    "        #[8, 4096, 40] * [8, 40, 77] -> [8, 4096, 77]\n",
    "        #atten = q.bmm(k.transpose(1, 2)) * (self.dim_q//8)**-0.5\n",
    "\n",
    "        #从数学上是等价的，但是在实际计算时会产生很小的误差\n",
    "        atten = torch.baddbmm(\n",
    "            torch.empty(q.shape[0], q.shape[1], k.shape[1], device=q.device),\n",
    "            q,\n",
    "            k.transpose(1, 2),\n",
    "            beta=0,\n",
    "            alpha=(self.dim_q//8)**-0.5,\n",
    "        )\n",
    "\n",
    "        atten = atten.softmax(dim=-1)\n",
    "\n",
    "        #[8, 4096, 77] * [8, 77, 40] -> [8, 4096, 40]\n",
    "        atten = atten.bmm(v)\n",
    "\n",
    "        def reshape(x):\n",
    "            #x: [8, 4096, 40]\n",
    "            b, lens, dim = x.shape\n",
    "\n",
    "            #[8, 4096, 40] -> [1, 8, 4096, 40]\n",
    "            x = x.reshape(b//8, 8, lens, dim)\n",
    "\n",
    "            #[1, 8, 4096, 40] -> [1, 4096, 8, 40]\n",
    "            x = x.transpose(1, 2)\n",
    "\n",
    "            #[1, 4096, 320]\n",
    "            x = x.reshape(b//8, lens, dim * 8)\n",
    "\n",
    "            return x\n",
    "\n",
    "        #[8, 4096, 40] -> [1, 4096, 320]\n",
    "        atten = reshape(atten)\n",
    "\n",
    "        #[1, 4096, 320] -> [1, 4096, 320]\n",
    "        atten = self.out(atten)\n",
    "\n",
    "        return atten\n",
    "\n",
    "CrossAttention(320, 768)(torch.randn(1, 4096, 320), torch.randn(1, 77, 768)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 320, 64, 64])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Transformer(torch.nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "\n",
    "        # in\n",
    "        self.norm_in = torch.nn.GroupNorm(\n",
    "            num_groups=32, num_channels=dim, eps=1e-6, affine=True\n",
    "        )\n",
    "\n",
    "        self.cnn_in = torch.nn.Conv2d(dim, dim, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        # atten\n",
    "        self.norm_atten0 = torch.nn.LayerNorm(dim, elementwise_affine=True)\n",
    "        self.atten1 = CrossAttention(dim, dim)\n",
    "        self.norm_atten1 = torch.nn.LayerNorm(dim, elementwise_affine=True)\n",
    "        self.atten2 = CrossAttention(dim, 768)\n",
    "\n",
    "        # act\n",
    "        self.norm_act = torch.nn.LayerNorm(dim, elementwise_affine=True)\n",
    "        self.fc0 = torch.nn.Linear(dim, dim * 8)\n",
    "        self.act = torch.nn.GELU()\n",
    "        self.fc1 = torch.nn.Linear(dim * 4, dim)\n",
    "\n",
    "        # out\n",
    "        self.cnn_out = torch.nn.Conv2d(dim, dim, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, q, kv):\n",
    "        # q: [1, 320, 64, 64]\n",
    "        # kv: [1, 77, 768]\n",
    "        b, _, h, w = q.shape\n",
    "        res1 = q\n",
    "\n",
    "        # ----in----\n",
    "        # 维度不变\n",
    "        # [1, 320, 64, 64]\n",
    "        q = self.cnn_in(self.norm_in(q))\n",
    "\n",
    "        # [1, 320, 64, 64] -> [1, 64, 64, 320] -> [1, 4096, 320]\n",
    "        q = q.permute(0, 2, 3, 1).reshape(b, h * w, self.dim)\n",
    "\n",
    "        # ----atten----\n",
    "        # 维度不变\n",
    "        # [1, 4096, 320]\n",
    "        q = self.atten1(q=self.norm_atten0(q), kv=self.norm_atten0(q)) + q\n",
    "        q = self.atten2(q=self.norm_atten1(q), kv=kv) + q\n",
    "\n",
    "        # ----act----\n",
    "        # [1, 4096, 320]\n",
    "        res2 = q\n",
    "\n",
    "        # [1, 4096, 320] -> [1, 4096, 2560]\n",
    "        q = self.fc0(self.norm_act(q))\n",
    "\n",
    "        # 1280\n",
    "        d = q.shape[2] // 2\n",
    "\n",
    "        # [1, 4096, 1280] * [1, 4096, 1280] -> [1, 4096, 1280]\n",
    "        # 这样做的意义是允许网络学习如何在不同特征表示之间进行信息流动的控制。\n",
    "        # 通过将特征空间一分为二，一半用来生成门控信号，另一半通过门控信号进行调节，\n",
    "        # 这种方法允许模型在不同的子空间中学习不同的特征表示，从而可能提高模型的表达能力。\n",
    "        q = q[:, :, :d] * self.act(q[:, :, d:])     #相当于FFN\n",
    "\n",
    "        # [1, 4096, 1280] -> [1, 4096, 320]\n",
    "        q = self.fc1(q) + res2\n",
    "\n",
    "        # ----out----\n",
    "        # [1, 4096, 320] -> [1, 64, 64, 320] -> [1, 320, 64, 64]\n",
    "        q = q.reshape(b, h, w, self.dim).permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "        # 维度不变\n",
    "        # [1, 320, 64 ,64]\n",
    "        q = self.cnn_out(q) + res1\n",
    "        return q\n",
    "\n",
    "\n",
    "Transformer(320)(torch.randn(1, 320, 64, 64), torch.randn(1, 77, 768)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 640, 16, 16])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DownBlock(torch.nn.Module):\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tf0 = Transformer(dim_out)\n",
    "        self.res0 = Resnet(dim_in, dim_out)\n",
    "\n",
    "        self.tf1 = Transformer(dim_out)\n",
    "        self.res1 = Resnet(dim_out, dim_out)\n",
    "\n",
    "        self.out = torch.nn.Conv2d(dim_out, dim_out, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, out_vae, out_encoder, time):\n",
    "        outs = []\n",
    "\n",
    "        out_vae = self.res0(out_vae, time)\n",
    "        out_vae = self.tf0(out_vae, out_encoder)\n",
    "        outs.append(out_vae)\n",
    "\n",
    "        out_vae = self.res1(out_vae, time)\n",
    "        out_vae = self.tf1(out_vae, out_encoder)\n",
    "        outs.append(out_vae)\n",
    "\n",
    "        out_vae = self.out(out_vae)\n",
    "        outs.append(out_vae)\n",
    "\n",
    "        return out_vae, outs\n",
    "\n",
    "\n",
    "DownBlock(320, 640)(\n",
    "    torch.randn(1, 320, 32, 32),\n",
    "    torch.randn(1, 77, 768),\n",
    "    torch.randn(1, 1280),\n",
    ")[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 640, 64, 64])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class UpBlock(torch.nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, dim_prev, add_up):\n",
    "        super().__init__()\n",
    "\n",
    "        self.res0 = Resnet(dim_out + dim_prev, dim_out)\n",
    "        self.res1 = Resnet(dim_out + dim_out, dim_out)\n",
    "        self.res2 = Resnet(dim_in + dim_out, dim_out)\n",
    "\n",
    "        self.tf0 = Transformer(dim_out)\n",
    "        self.tf1 = Transformer(dim_out)\n",
    "        self.tf2 = Transformer(dim_out)\n",
    "\n",
    "        self.out = None\n",
    "        if add_up:\n",
    "            self.out = torch.nn.Sequential(\n",
    "                torch.nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
    "                torch.nn.Conv2d(dim_out, dim_out, kernel_size=3, padding=1),\n",
    "            )\n",
    "\n",
    "    def forward(self, out_vae, out_encoder, time, out_down):\n",
    "        out_vae = self.res0(torch.cat([out_vae, out_down.pop()], dim=1), time)\n",
    "        out_vae = self.tf0(out_vae, out_encoder)\n",
    "\n",
    "        out_vae = self.res1(torch.cat([out_vae, out_down.pop()], dim=1), time)\n",
    "        out_vae = self.tf1(out_vae, out_encoder)\n",
    "\n",
    "        out_vae = self.res2(torch.cat([out_vae, out_down.pop()], dim=1), time)\n",
    "        out_vae = self.tf2(out_vae, out_encoder)\n",
    "\n",
    "        if self.out:\n",
    "            out_vae = self.out(out_vae)\n",
    "\n",
    "        return out_vae\n",
    "\n",
    "\n",
    "UpBlock(320, 640, 1280, True)(\n",
    "    torch.randn(1, 1280, 32, 32),\n",
    "    torch.randn(1, 77, 768),\n",
    "    torch.randn(1, 1280),\n",
    "    [\n",
    "        torch.randn(1, 320, 32, 32),\n",
    "        torch.randn(1, 640, 32, 32),\n",
    "        torch.randn(1, 640, 32, 32),\n",
    "    ],\n",
    ").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 64, 64])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class UNet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        #in\n",
    "        self.in_vae = torch.nn.Conv2d(4, 320, kernel_size=3, padding=1)\n",
    "\n",
    "        self.in_time = torch.nn.Sequential(\n",
    "            torch.nn.Linear(320, 1280),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(1280, 1280),\n",
    "        )\n",
    "\n",
    "        #down\n",
    "        self.down_block0 = DownBlock(320, 320)\n",
    "        self.down_block1 = DownBlock(320, 640)\n",
    "        self.down_block2 = DownBlock(640, 1280)\n",
    "\n",
    "        self.down_res0 = Resnet(1280, 1280)\n",
    "        self.down_res1 = Resnet(1280, 1280)\n",
    "\n",
    "        #mid\n",
    "        self.mid_res0 = Resnet(1280, 1280)\n",
    "        self.mid_tf = Transformer(1280)\n",
    "        self.mid_res1 = Resnet(1280, 1280)\n",
    "\n",
    "        #up\n",
    "        self.up_res0 = Resnet(2560, 1280)\n",
    "        self.up_res1 = Resnet(2560, 1280)\n",
    "        self.up_res2 = Resnet(2560, 1280)\n",
    "\n",
    "        self.up_in = torch.nn.Sequential(\n",
    "            torch.nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            torch.nn.Conv2d(1280, 1280, kernel_size=3, padding=1),\n",
    "        )\n",
    "\n",
    "        self.up_block0 = UpBlock(640, 1280, 1280, True)\n",
    "        self.up_block1 = UpBlock(320, 640, 1280, True)\n",
    "        self.up_block2 = UpBlock(320, 320, 640, False)\n",
    "\n",
    "        #out\n",
    "        self.out = torch.nn.Sequential(\n",
    "            torch.nn.GroupNorm(num_channels=320, num_groups=32, eps=1e-5),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Conv2d(320, 4, kernel_size=3, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, out_vae, out_encoder, time):\n",
    "        #out_vae -> [2, 4, 64, 64]\n",
    "        #out_encoder -> [2, 77, 768]\n",
    "        #time -> [1]\n",
    "\n",
    "        #----in----\n",
    "        #[2, 4, 64, 64] -> [2, 320, 64, 64]\n",
    "        out_vae = self.in_vae(out_vae)\n",
    "\n",
    "        def get_time_embed(t):\n",
    "            #-9.210340371976184 = -math.log(10000)\n",
    "            e = torch.arange(160) * -9.210340371976184 / 160\n",
    "            e = e.exp().to(t.device) * t\n",
    "\n",
    "            #[160+160] -> [320] -> [1, 320]\n",
    "            e = torch.cat([e.cos(), e.sin()]).unsqueeze(dim=0)\n",
    "\n",
    "            return e\n",
    "\n",
    "        #[1] -> [1, 320]\n",
    "        time = get_time_embed(time)\n",
    "        #[1, 320] -> [1, 1280]\n",
    "        time = self.in_time(time)\n",
    "\n",
    "        #----down----\n",
    "        #[2, 320, 64, 64]\n",
    "        #[2, 320, 64, 64]\n",
    "        #[2, 320, 64, 64]\n",
    "        #[2, 320, 32, 32]\n",
    "        #[2, 640, 32, 32]\n",
    "        #[2, 640, 32, 32]\n",
    "        #[2, 640, 16, 16]\n",
    "        #[2, 1280, 16, 16]\n",
    "        #[2, 1280, 16, 16]\n",
    "        #[2, 1280, 8, 8]\n",
    "        #[2, 1280, 8, 8]\n",
    "        #[2, 1280, 8, 8]\n",
    "        out_down = [out_vae]\n",
    "\n",
    "        #[2, 320, 64, 64],[2, 77, 768],[1, 1280] -> [2, 320, 32, 32]\n",
    "        #out -> [2, 320, 64, 64],[2, 320, 64, 64][2, 320, 32, 32]\n",
    "        out_vae, out = self.down_block0(out_vae=out_vae,\n",
    "                                        out_encoder=out_encoder,\n",
    "                                        time=time)\n",
    "        out_down.extend(out)\n",
    "\n",
    "        #[2, 320, 32, 32],[2, 77, 768],[1, 1280] -> [2, 640, 16, 16]\n",
    "        #out -> [2, 640, 32, 32],[2, 640, 32, 32],[2, 640, 16, 16]\n",
    "        out_vae, out = self.down_block1(out_vae=out_vae,\n",
    "                                        out_encoder=out_encoder,\n",
    "                                        time=time)\n",
    "        out_down.extend(out)\n",
    "\n",
    "        #[2, 640, 16, 16],[2, 77, 768],[1, 1280] -> [2, 1280, 8, 8]\n",
    "        #out -> [2, 1280, 16, 16],[2, 1280, 16, 16],[2, 1280, 8, 8]\n",
    "        out_vae, out = self.down_block2(out_vae=out_vae,\n",
    "                                        out_encoder=out_encoder,\n",
    "                                        time=time)\n",
    "        out_down.extend(out)\n",
    "\n",
    "        #[2, 1280, 8, 8],[1, 1280] -> [2, 1280, 8, 8]\n",
    "        out_vae = self.down_res0(out_vae, time)\n",
    "        out_down.append(out_vae)\n",
    "\n",
    "        #[2, 1280, 8, 8],[1, 1280] -> [2, 1280, 8, 8]\n",
    "        out_vae = self.down_res1(out_vae, time)\n",
    "        out_down.append(out_vae)\n",
    "\n",
    "        #----mid----\n",
    "        #[2, 1280, 8, 8],[1, 1280] -> [2, 1280, 8, 8]\n",
    "        out_vae = self.mid_res0(out_vae, time)\n",
    "\n",
    "        #[2, 1280, 8, 8],[2, 77, 768] -> [2, 1280, 8, 8]\n",
    "        out_vae = self.mid_tf(out_vae, out_encoder)\n",
    "\n",
    "        #[2, 1280, 8, 8],[1, 1280] -> [2, 1280, 8, 8]\n",
    "        out_vae = self.mid_res1(out_vae, time)\n",
    "\n",
    "        #----up----\n",
    "        #[2, 1280+1280, 8, 8],[1, 1280] -> [2, 1280, 8, 8]\n",
    "        out_vae = self.up_res0(torch.cat([out_vae, out_down.pop()], dim=1),\n",
    "                                time)\n",
    "\n",
    "        #[2, 1280+1280, 8, 8],[1, 1280] -> [2, 1280, 8, 8]\n",
    "        out_vae = self.up_res1(torch.cat([out_vae, out_down.pop()], dim=1),\n",
    "                                time)\n",
    "\n",
    "        #[2, 1280+1280, 8, 8],[1, 1280] -> [2, 1280, 8, 8]\n",
    "        out_vae = self.up_res2(torch.cat([out_vae, out_down.pop()], dim=1),\n",
    "                                time)\n",
    "\n",
    "        #[2, 1280, 8, 8] -> [2, 1280, 16, 16]\n",
    "        out_vae = self.up_in(out_vae)\n",
    "\n",
    "        #[2, 1280, 16, 16],[2, 77, 768],[1, 1280] -> [2, 1280, 32, 32]\n",
    "        #out_down -> [2, 640, 16, 16],[2, 1280, 16, 16],[2, 1280, 16, 16]\n",
    "        out_vae = self.up_block0(out_vae=out_vae,\n",
    "                                out_encoder=out_encoder,\n",
    "                                time=time,\n",
    "                                out_down=out_down)\n",
    "\n",
    "        #[2, 1280, 32, 32],[2, 77, 768],[1, 1280] -> [2, 640, 64, 64]\n",
    "        #out_down -> [2, 320, 32, 32],[2, 640, 32, 32],[2, 640, 32, 32]\n",
    "        out_vae = self.up_block1(out_vae=out_vae,\n",
    "                                out_encoder=out_encoder,\n",
    "                                time=time,\n",
    "                                out_down=out_down)\n",
    "\n",
    "        #[2, 640, 64, 64],[2, 77, 768],[1, 1280] -> [2, 320, 64, 64]\n",
    "        #out_down -> [2, 320, 64, 64],[2, 320, 64, 64],[2, 320, 64, 64]\n",
    "        out_vae = self.up_block2(out_vae=out_vae,\n",
    "                                out_encoder=out_encoder,\n",
    "                                time=time,\n",
    "                                out_down=out_down)\n",
    "\n",
    "        #----out----\n",
    "        #[2, 320, 64, 64] -> [2, 4, 64, 64]\n",
    "        out_vae = self.out(out_vae)\n",
    "\n",
    "        return out_vae\n",
    "\n",
    "\n",
    "UNet()(torch.randn(2, 4, 64, 64), torch.randn(2, 77, 768),\n",
    "        torch.LongTensor([26])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from diffusers import UNet2DConditionModel\n",
    "\n",
    "#加载预训练模型的参数\n",
    "params = UNet2DConditionModel.from_pretrained(\n",
    "    './Diffusion_model', subfolder='unet')\n",
    "\n",
    "unet = UNet()\n",
    "\n",
    "#in\n",
    "unet.in_vae.load_state_dict(params.conv_in.state_dict())\n",
    "unet.in_time[0].load_state_dict(params.time_embedding.linear_1.state_dict())\n",
    "unet.in_time[2].load_state_dict(params.time_embedding.linear_2.state_dict())\n",
    "\n",
    "\n",
    "#down\n",
    "def load_tf(model, param):\n",
    "    model.norm_in.load_state_dict(param.norm.state_dict())\n",
    "    model.cnn_in.load_state_dict(param.proj_in.state_dict())\n",
    "\n",
    "    model.atten1.q.load_state_dict(\n",
    "        param.transformer_blocks[0].attn1.to_q.state_dict())\n",
    "    model.atten1.k.load_state_dict(\n",
    "        param.transformer_blocks[0].attn1.to_k.state_dict())\n",
    "    model.atten1.v.load_state_dict(\n",
    "        param.transformer_blocks[0].attn1.to_v.state_dict())\n",
    "    model.atten1.out.load_state_dict(\n",
    "        param.transformer_blocks[0].attn1.to_out[0].state_dict())\n",
    "\n",
    "    model.atten2.q.load_state_dict(\n",
    "        param.transformer_blocks[0].attn2.to_q.state_dict())\n",
    "    model.atten2.k.load_state_dict(\n",
    "        param.transformer_blocks[0].attn2.to_k.state_dict())\n",
    "    model.atten2.v.load_state_dict(\n",
    "        param.transformer_blocks[0].attn2.to_v.state_dict())\n",
    "    model.atten2.out.load_state_dict(\n",
    "        param.transformer_blocks[0].attn2.to_out[0].state_dict())\n",
    "\n",
    "    model.fc0.load_state_dict(\n",
    "        param.transformer_blocks[0].ff.net[0].proj.state_dict())\n",
    "\n",
    "    model.fc1.load_state_dict(\n",
    "        param.transformer_blocks[0].ff.net[2].state_dict())\n",
    "\n",
    "    model.norm_atten0.load_state_dict(\n",
    "        param.transformer_blocks[0].norm1.state_dict())\n",
    "    model.norm_atten1.load_state_dict(\n",
    "        param.transformer_blocks[0].norm2.state_dict())\n",
    "    model.norm_act.load_state_dict(\n",
    "        param.transformer_blocks[0].norm3.state_dict())\n",
    "\n",
    "    model.cnn_out.load_state_dict(param.proj_out.state_dict())\n",
    "\n",
    "\n",
    "def load_res(model, param):\n",
    "    model.time[1].load_state_dict(param.time_emb_proj.state_dict())\n",
    "\n",
    "    model.s0[0].load_state_dict(param.norm1.state_dict())\n",
    "    model.s0[2].load_state_dict(param.conv1.state_dict())\n",
    "\n",
    "    model.s1[0].load_state_dict(param.norm2.state_dict())\n",
    "    model.s1[2].load_state_dict(param.conv2.state_dict())\n",
    "\n",
    "    if isinstance(model.res, torch.nn.Module):\n",
    "        model.res.load_state_dict(param.conv_shortcut.state_dict())\n",
    "\n",
    "\n",
    "def load_down_block(model, param):\n",
    "    load_tf(model.tf0, param.attentions[0])\n",
    "    load_tf(model.tf1, param.attentions[1])\n",
    "\n",
    "    load_res(model.res0, param.resnets[0])\n",
    "    load_res(model.res1, param.resnets[1])\n",
    "\n",
    "    model.out.load_state_dict(param.downsamplers[0].conv.state_dict())\n",
    "\n",
    "\n",
    "load_down_block(unet.down_block0, params.down_blocks[0])\n",
    "load_down_block(unet.down_block1, params.down_blocks[1])\n",
    "load_down_block(unet.down_block2, params.down_blocks[2])\n",
    "\n",
    "load_res(unet.down_res0, params.down_blocks[3].resnets[0])\n",
    "load_res(unet.down_res1, params.down_blocks[3].resnets[1])\n",
    "\n",
    "#mid\n",
    "load_tf(unet.mid_tf, params.mid_block.attentions[0])\n",
    "load_res(unet.mid_res0, params.mid_block.resnets[0])\n",
    "load_res(unet.mid_res1, params.mid_block.resnets[1])\n",
    "\n",
    "#up\n",
    "load_res(unet.up_res0, params.up_blocks[0].resnets[0])\n",
    "load_res(unet.up_res1, params.up_blocks[0].resnets[1])\n",
    "load_res(unet.up_res2, params.up_blocks[0].resnets[2])\n",
    "unet.up_in[1].load_state_dict(\n",
    "    params.up_blocks[0].upsamplers[0].conv.state_dict())\n",
    "\n",
    "\n",
    "def load_up_block(model, param):\n",
    "    load_tf(model.tf0, param.attentions[0])\n",
    "    load_tf(model.tf1, param.attentions[1])\n",
    "    load_tf(model.tf2, param.attentions[2])\n",
    "\n",
    "    load_res(model.res0, param.resnets[0])\n",
    "    load_res(model.res1, param.resnets[1])\n",
    "    load_res(model.res2, param.resnets[2])\n",
    "\n",
    "    if isinstance(model.out, torch.nn.Module):\n",
    "        model.out[1].load_state_dict(param.upsamplers[0].conv.state_dict())\n",
    "\n",
    "\n",
    "load_up_block(unet.up_block0, params.up_blocks[1])\n",
    "load_up_block(unet.up_block1, params.up_blocks[2])\n",
    "load_up_block(unet.up_block2, params.up_blocks[3])\n",
    "\n",
    "#out\n",
    "unet.out[0].load_state_dict(params.conv_norm_out.state_dict())\n",
    "unet.out[2].load_state_dict(params.conv_out.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_vae = torch.randn(1, 4, 64, 64)\n",
    "out_encoder = torch.randn(1, 77, 768)\n",
    "time = torch.LongTensor([26])\n",
    "\n",
    "a = unet(out_vae=out_vae, out_encoder=out_encoder, time=time)\n",
    "b = params(out_vae, time, out_encoder).sample\n",
    "\n",
    "(a == b).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
