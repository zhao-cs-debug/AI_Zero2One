{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "效果完全没有区别，只是实现方式不一样。\n",
    "\n",
    "shuffle=True 的实现方式： 在每个 epoch 开始时将整个数据集打乱，然后按照打乱后的顺序划分 batch。再按照batch_size 个数依次提取数据 \n",
    "\n",
    "sampler.BatchSampler(random_sampler) 的实现方式：（数据不会打乱）\n",
    "\n",
    "step 1、RandomSampler 会生成随机的索引。\n",
    "\n",
    "step 2、BatchSampler 根据上面随机出来的索引生成 batch 组。\n",
    "\n",
    "step 3、拿着每个batch 组的索引去取 数据\n",
    "\n",
    "相同点：\n",
    "\n",
    "每个epoch 都会重新打乱\n",
    "\n",
    "都不会重复采样，除非你通过参数指定了可以重复采样  \n",
    "\n",
    "其他说明：\n",
    "\n",
    "shuffle=True 的性能更高一些，而 BatchSampler灵活性更高，因为你可以通过 BatchSampler 设计更复杂的采样方式\n",
    "\n",
    "在 Dataloader 中使用 batch_sampler 的常见目的之一，是为了兼容 DistributedSampler，比如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.distributed:\n",
    "    sampler_train = DistributedSampler(dataset_train)\n",
    "    sampler_val = DistributedSampler(dataset_val, shuffle=False)\n",
    "else:\n",
    "    sampler_train = torch.utils.data.RandomSampler(dataset_train)\n",
    "    sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
    "\n",
    "batch_sampler_train = torch.utils.data.BatchSampler(sampler_train, args.batch_size, drop_last=True)\n",
    "\n",
    "data_loader_train = DataLoader(dataset_train,\n",
    "                               batch_sampler=batch_sampler_train,\n",
    "                               collate_fn=utils.collate_fn,\n",
    "                               )\n",
    "data_loader_val = DataLoader(dataset_val,\n",
    "                             args.batch_size,\n",
    "                             sampler=sampler_val,\n",
    "                             drop_last=False,\n",
    "                             collate_fn=utils.collate_fn,\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "跑个小例子，看一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data.sampler as sampler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.data = [1, 2, 3, 4, 5]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "\n",
    "dataset = MyDataset()\n",
    "\n",
    "# =============================================\n",
    "random_sampler = sampler.RandomSampler(data_source=dataset)\n",
    "batch_sampler = sampler.BatchSampler(random_sampler, batch_size=2, drop_last=False)\n",
    "dataloader1 = DataLoader(dataset, batch_sampler=batch_sampler)\n",
    "\n",
    "for epoch in range(3):\n",
    "    for index, data in enumerate(dataloader1):\n",
    "        print(index, data)\n",
    "print('*'*30)\n",
    "\n",
    "# =============================================\n",
    "dataloader2 = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "for epoch in range(3):\n",
    "    for index, data in enumerate(dataloader2):\n",
    "        print(index, data)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
