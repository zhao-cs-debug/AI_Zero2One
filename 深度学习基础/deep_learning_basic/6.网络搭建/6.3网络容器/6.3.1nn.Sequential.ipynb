{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nn.Sequential\n",
    "## 简介\n",
    "torch.nn.Sequential 是 PyTorch 中的一个容器模块，它提供了一种简单的方式来构建神经网络模型。\n",
    "\n",
    "以下是 torch.nn.Sequential 的主要特点和使用方法：\n",
    "1. 简化模型构建： Sequential 允许我们按照顺序，往网络中添加一系列的子模块，且无需再定义 forward 方法（其内部已实现）\n",
    "2. 顺序执行： 子模块将按照它们在 Sequential 中的添加顺序依次执行。输入数据将按顺序经过每个子模块，上一个子模块的输出将作为 下一个子模块的输入\n",
    "3. 与普通模块的一致性： Sequential 本身也是 nn.Module 的子类，因此你可以像对待其他模块一样对待它"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用举例1\n",
    "import torch.nn as nn\n",
    "\n",
    "net = nn.Sequential(nn.Linear(10, 20),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(20, 30))\n",
    "print(net)\n",
    "\n",
    "# 这里，Sequential 包含了两个线性层和一个 ReLU 激活函数，按照顺序执行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用举例 2\n",
    "# 用命名模块： 你也可以使用命名模块来使代码更加清晰\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "net = nn.Sequential(OrderedDict([('linear1', nn.Linear(10, 20)),\n",
    "                                 ('relu', nn.ReLU()),\n",
    "                                 ('linear2', nn.Linear(20, 30))]))\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 之后我们可以通过 net.linear1、net.relu、net.linear2 来分别访问每个子模块。\n",
    "\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "net = nn.Sequential(OrderedDict([('linear1', nn.Linear(10, 20)),\n",
    "                                 ('relu', nn.ReLU()),\n",
    "                                 ('linear2', nn.Linear(20, 30))]))\n",
    "print(net.linear1)\n",
    "print(net.relu)\n",
    "print(net.linear2)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
