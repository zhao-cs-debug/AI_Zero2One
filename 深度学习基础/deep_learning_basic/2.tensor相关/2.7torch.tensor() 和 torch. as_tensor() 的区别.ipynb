{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在跑模型的时候，大家可能会遇到如下报错\n",
    "\n",
    "UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
    "\n",
    "这时只要将 torch.tensor()  改写成  torch.as_tensor() 就可以避免报错了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如下写法报错\n",
    " feature = torch.tensor(image, dtype=torch.float32)\n",
    "\n",
    "# 改为\n",
    "feature = torch.as_tensor(image, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1、torch.as_tensor()\n",
    "\n",
    "new_data = torch.as_tensor(data, dtype=None,device=None)\n",
    "\n",
    "作用：生成一个新的 tensor， 这个新生成的tensor 会根据原数据的实际情况，来决定是进行浅拷贝，还是深拷贝。当然，会优先浅拷贝，浅拷贝会共享内存，并共享 autograd 历史记录。\n",
    "\n",
    "## 情况一：不指定新生成的 tensor 的数据类型 和 device ，那么默认新生成的 tensor 的 数据类型 和 device 同原数据的 数据类型 和 device 相同，他会帮我们进行浅拷贝，并共享内存\n",
    "\n",
    "### 1）原数据为 array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "a = np.array([1, 2, 3])\n",
    "t = torch.as_tensor(a)\n",
    "t[0] = -1\n",
    "\n",
    "print(a)   # [-1  2  3]\n",
    "print(a.dtype)   # int64\n",
    "print(a.ctypes.data)   # 105553154015584\n",
    "\n",
    "print(t)   # tensor([-1,  2,  3])\n",
    "print(t.dtype)   # torch.int64\n",
    "print(t.untyped_storage().data_ptr())   # 105553154015584"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2）原数据为 tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([1., 2., 3.], device=torch.device('cuda'))\n",
    "a.grad = torch.tensor([5., 6., 7.], device=torch.device('cuda'))\n",
    "\n",
    "t = torch.as_tensor(a)\n",
    "t[0] = -1\n",
    "\n",
    "print(a)   # tensor([-1.,  2.,  3.], device='cuda:0')\n",
    "print(a.dtype)   # torch.float32\n",
    "\n",
    "print(t)   # tensor([-1.,  2.,  3.], device='cuda:0')\n",
    "print(t.dtype)   # torch.float32\n",
    "print(t.grad)   # tensor([5., 6., 7.], device='cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 情况二：  指定新生成的 tensor 的 数据类型 或 device 和原数据不一致，则会进行 深拷贝，不再共享内存\n",
    "\n",
    "### 1）数据类型相同，但是device不同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "a = np.array([1, 2, 3])\n",
    "t = torch.as_tensor(a, device=torch.device('cuda'))\n",
    "t[0] = -1\n",
    "\n",
    "print(a)   # [1 2 3]\n",
    "print(a.dtype)   # int64\n",
    "print(a.ctypes.data)   # 102285095257904\n",
    "\n",
    "print(t)   # tensor([-1,  2,  3], device='cuda:0')\n",
    "print(t.dtype)   # torch.int64\n",
    "print(t.untyped_storage().data_ptr())   # 138156835864576"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2）device相同，但数据类型不同，深拷贝，不再共享内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "a = np.array([1, 2, 3])\n",
    "t = torch.as_tensor(a, dtype=torch.float32)\n",
    "t[0] = -1\n",
    "\n",
    "print(a)   # [1 2 3]\n",
    "print(a.dtype)   # int64\n",
    "print(t)   # tensor([-1.,  2.,  3.])\n",
    "print(t.dtype)   # torch.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2、torch.tensor()\n",
    "torch.tensor() 是深拷贝方式。\n",
    "\n",
    "torch.tensor(data, dtype=None, device=None, requires_grad=False, pin_memory=False)\n",
    "\n",
    "深拷贝：会拷贝 数据类型 和 device， (对于叶子 tensor，also known as “leaf tensor”）不会记录 autograd 历史\n",
    "\n",
    "## 1）如果原数据的数据类型是 tensor，使用 torch.tensor(data) 就会报waring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 原数据类型是：tensor 会发出警告\n",
    "import torch\n",
    "\n",
    "a = torch.tensor([1, 2, 3], device=torch.device('cuda'))\n",
    "t = torch.tensor(a)\n",
    "t[0] = -1\n",
    "\n",
    "print(a)  # tensor([1, 2, 3], device='cuda:0')\n",
    "print(t)  # tensor([-1,  2,  3], device='cuda:0')\n",
    "\n",
    "# 输出：\n",
    "# tensor([1, 2, 3], device='cuda:0')\n",
    "# tensor([-1,  2,  3], device='cuda:0')\n",
    "# /opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor,\n",
    "# it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True),\n",
    "# rather than torch.tensor(sourceTensor)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2）如果原数据的数据类型是：list, tuple, NumPy ndarray, scalar, and other types，不会 waring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 原数据类型是：list, tuple, NumPy ndarray, scalar, and other types， 没警告\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "a =  np.array([1, 2, 3])\n",
    "t = torch.tensor(a)\n",
    "\n",
    "b = [1,2,3]\n",
    "t= torch.tensor(b)\n",
    "\n",
    "c = (1,2,3)\n",
    "t= torch.tensor(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
