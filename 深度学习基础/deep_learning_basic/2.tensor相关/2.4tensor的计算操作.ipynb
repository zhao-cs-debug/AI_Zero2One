{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tensor的计算操作\n",
    "## 1、创建tensor\n",
    "常见创建 tensor 的方法\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 你可以将如下生成的 tensor，逐个打印，进行观察\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "a = torch.Tensor(2, 3)     # 通过指定尺寸，生成一个 值全为 0 的 tensor\n",
    "b = torch.tensor([[1, 2, 3], [4, 5, 6]])   # 直接通过指定数据，生成tensor，支持 List、Numpy数组\n",
    "\n",
    "c = torch.eye(3, 3)        # 按照指定的行列数，生成二维单位tensor\n",
    "\n",
    "d = torch.rand(2, 3)       # 从 (0, 1) 之间，进行均匀分布采样，生成指定size 的tensor\n",
    "e = torch.randn(2, 3)      # 从标准正态分布中采样，生成指定size 的tensor\n",
    "\n",
    "f = torch.ones(3, 2)       # 按照指定 size，生成 值全为1 的 tensor\n",
    "g = torch.zeros(2, 3)      # 按照指定 size，生成 值全为0 的 tensor\n",
    "h = torch.ones_like(b)     # 返回尺寸与 t 相同的，值全为1 的 tensor\n",
    "i = torch.zeros_like(b)    # 返回尺寸与 t 相同的，值全为0 的 tensor\n",
    "\n",
    "j = torch.linspace(1, 10, 4)   # 从 start 到 end，均匀切分成 steps 份\n",
    "k = torch.arange(1, 5, 2)      # 在区间[start, end) 上，每间隔 step 生成一个序列张量\n",
    "\n",
    "l = np.arange(1, 5)\n",
    "m = torch.from_numpy(l)     # 根据 ndarray 生成 tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、查看tensor的形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(a.numel())   # 统计 tensor 元素的个数\n",
    "print(a.size())    # 获取 tensor 的尺寸，tensor.size() 是一个方法\n",
    "print(a.shape)     # 获取 tensor的尺寸，tensor.shape 是一个属性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3、修改 tensor 形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "print(a.view(3, 2))        # 修改 tensor 的尺寸，返回的对象 与 源tensor 共享内存（修改一个，另一个也会被修改）\n",
    "print(a.resize(3, 2))      # 修改 tensor 的尺寸，类似于view，但在size 超出时会重新分配内存空间\n",
    "print(a.reshape(3, 2))     # 修改 tensor 的尺寸，返回的对象是一个新生成的tensor，且不要求源tensor 是连续的\n",
    "print(a.flatten())         # 将张量扁平化为一维张量\n",
    "\n",
    "import torch\n",
    "\n",
    "a = torch.tensor([[[1, 2, 3], [4, 5, 6]]])\n",
    "print(a.shape)\n",
    "\n",
    "b = torch.unsqueeze(a, 0)  # 在指定维度增加一个 “1”\n",
    "print(b.shape)\n",
    "\n",
    "c = torch.squeeze(b, 0)    # 在指定维度删除一个 “1”\n",
    "print(c.shape)\n",
    "\n",
    "import torch\n",
    "\n",
    "a = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "b = a.transpose(dim0=0, dim1=1)    # 交换 dim0 和 dim1 指定的两个维度，返回一个新的张量而不修改原始张量\n",
    "print(a.shape)\n",
    "print(b.shape)\n",
    "\n",
    "c = torch.tensor([[[1, 2, 2], [3, 4, 3]]])\n",
    "d = c.permute(1, 2, 0)             # 按照 dims 指定的顺序重新排列张量的维度，返回一个新的张量而不修改原始张量\n",
    "print(c.shape)\n",
    "print(d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4、按条件筛选\n",
    "### 1）index_select\n",
    "torch.index_select(input, dim, index) : 从输入张量中按索引选择某些元素，并返回一个新的张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "tensor = torch.tensor([[1, 2, 3],\n",
    "                       [4, 5, 6],\n",
    "                       [7, 8, 9]])\n",
    "\n",
    "# 创建一个要选择的索引张量\n",
    "indices = torch.tensor([0, 2])\n",
    "\n",
    "# 在第0维上使用index_select选择索引为0和2的行\n",
    "selected_rows = torch.index_select(tensor, 0, indices)\n",
    "\n",
    "print(\"Selected rows:\\n\", selected_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2）nonzero\n",
    "torch.nonzero(input) : 获取张量中非零元素的索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "tensor = torch.tensor([[0, 1, 0],\n",
    "                       [2, 0, 3],\n",
    "                       [0, 4, 0]])\n",
    "\n",
    "# 获取非零元素的索引\n",
    "nonzero_indices = torch.nonzero(tensor)\n",
    "print(\"Non-zero indices:\\n\", nonzero_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3）masked_select\n",
    "torch.masked_select(input, masked) ： 根据掩码（mask）从输入张量中选择元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 创建一个示例张量\n",
    "input_tensor = torch.tensor([[1, 2, 3],\n",
    "                             [4, 5, 6],\n",
    "                             [7, 8, 9]])\n",
    "\n",
    "# 创建一个掩码张量\n",
    "mask_tensor = torch.tensor([[0, 1, 0],\n",
    "                            [1, 0, 1],\n",
    "                            [0, 1, 0]], dtype=torch.bool)\n",
    "\n",
    "# 使用掩码选择张量中的元素\n",
    "selected_tensor = torch.masked_select(input_tensor, mask_tensor)\n",
    "print(\"Selected tensor:\\n\", selected_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4）gather\n",
    "torch.gather(input, dim, index) :  在指定维度上根据索引从输入张量中选择元素。若 dim=n ：\n",
    "\n",
    "要求除了第n个维度，input_tensor 和 index_tensor 其他维度数量一致\n",
    "\n",
    "在第n个维度上，通过 index_tensor 指定的索引，从 input_tensor 中取出对应位置的元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 创建一个示例张量\n",
    "input_tensor = torch.tensor([[1, 2, 3],\n",
    "                             [4, 5, 6],\n",
    "                             [7, 8, 9]])\n",
    "\n",
    "# 创建一个示例索引张量\n",
    "index_tensor = torch.tensor([[0, 2],\n",
    "                             [1, 0],\n",
    "                             [2, 1]])\n",
    "\n",
    "# 在第1维度上使用索引张量收集值\n",
    "gathered_tensor = torch.gather(input_tensor, 1, index_tensor)\n",
    "\n",
    "print(\"Gathered tensor:\\n\", gathered_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5）scatter\n",
    "torch.scatter(input, dim, index, src) ：在指定维度（dim）上根据指定的索引（index），将源张量（src）的值放到目标张量（input）中\n",
    "\n",
    "src 的形状 和 index的形状 必须保持一致，否则会报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 创建一个示例目标张量\n",
    "input_tensor = torch.zeros(4, 4)\n",
    "\n",
    "# 创建一个示例索引张量\n",
    "index_tensor = torch.tensor([[0, 1],\n",
    "                             [2, 3]])\n",
    "\n",
    "# 创建一个示例源张量\n",
    "src_tensor = torch.tensor([[1, 2],\n",
    "                           [3, 4]])\n",
    "\n",
    "# 在第1维度上使用索引张量散射值\n",
    "scattered_tensor = torch.scatter(input_tensor, 1, index_tensor, src_tensor)\n",
    "print(\"Scattered tensor:\\n\", scattered_tensor)\n",
    "\n",
    "# No. 1\n",
    "# src_tensor 第0行第0列的值为“1”\n",
    "# index_tensor  第0行第0列的值为“0”， dim=1 表示 “0” 是列索引\n",
    "# 将 “1” 放到 input_tensor 中，对应行第“0”列的位置上\n",
    "# No. 2\n",
    "# src_tensor 第0行第1列的值为“2”，\n",
    "# index_tensor  第0行第1列的值为“1”，dim=1 表示 “1” 是列索引\n",
    "# 将 “2” 放到 input_tensor 中，对应行第“1”列的位置上\n",
    "# No. 3\n",
    "# src_tensor 第1行第0列的值为“3”，\n",
    "# index_tensor  第1行第0列的值为“2”，dim=1 表示 “2” 是列索引\n",
    "# 将 “3” 放到 input_tensor 中，对应行第“2”列的位置上\n",
    "# No. 4\n",
    "# src_tensor 第1行第1列的值为“4”，\n",
    "# index_tensor  第1行第1列的值为“3”，dim=1 表示 “3” 是列索引\n",
    "# 将 “4” 放到 input_tensor 中，对应行第“3”列的位置上"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5、运算操作\n",
    "### 1）abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.arange(-5, 5)\n",
    "y = torch.abs(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2）add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.arange(2, 5)\n",
    "y = torch.arange(4, 7)\n",
    "z = torch.add(x, y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3）addcdiv\n",
    "torch.addcdiv(input, tensor1, tensor2, value)\n",
    "\n",
    "result = input + value ∗ tensor2 / tensor1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "t = torch.randn(1, 3)\n",
    "t1 = torch.randn(3, 1)\n",
    "t2 = torch.randn(1, 3)\n",
    "\n",
    "a = t + 0.1 *(t1 / t2)\n",
    "print(a)\n",
    "\n",
    "b = torch.addcdiv(t, t1, t2, value=0.1)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4）addcmul\n",
    "torch.addcmul(input, tensor1, tensor2, value)\n",
    "\n",
    "result=input+value∗tensor1∗tensor2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "t = torch.randn(1, 3)\n",
    "t1 = torch.randn(3, 1)\n",
    "t2 = torch.randn(1, 3)\n",
    "\n",
    "a = t + 0.1 * t1 * t2\n",
    "print(a)\n",
    "\n",
    "b = torch.addcmul(t, t1, t2, value=0.1)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5）ceil、floor\n",
    "torch.ceil(input) ：向上取整\n",
    "\n",
    "torch.floor(input) ：向下取整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(8)\n",
    "x = torch.randn(3) * 10\n",
    "y = torch.ceil(x)\n",
    "z = torch.floor(x)\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6）clamp\n",
    "将张量元素大小限制在指定区间范围内"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.arange(1, 8)\n",
    "y = torch.clamp(x, 2, 5)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7）exp、log、pow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(8)\n",
    "x = torch.arange(3)\n",
    "print(x)\n",
    "print(torch.exp(x))\n",
    "print(torch.log(x))  # 以e为底\n",
    "print(torch.pow(x, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8）mul\n",
    "逐元素乘法，效果和 * 一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([[2, 2, 2],\n",
    "                  [2, 3, 4]])\n",
    "\n",
    "b = torch.tensor([[3, 3, 3],\n",
    "                  [4, 5, 6]])\n",
    "\n",
    "print(torch.mul(a,b))\n",
    "print(a*b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9）neg、sqrt、sign\n",
    "torch.neg()    取反\n",
    "\n",
    "torch.sqrt()   开根号\n",
    "\n",
    "torch.sign()   取符号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([[2, -2, 2],\n",
    "                  [2, -3, 4]])\n",
    "\n",
    "print(torch.neg(a))\n",
    "\n",
    "print(torch.sqrt(a))\n",
    "\n",
    "print(torch.sign(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6、统计操作\n",
    "### 1）sum、prod、cumsum、cumprod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.linspace(0, 10, 6).view(2, 3)\n",
    "print(a)\n",
    "\n",
    "b = a.sum(dim=0)\n",
    "c = torch.cumsum(a, dim=0)\n",
    "print('\\n维度0上求和 与 累加')\n",
    "print(b)\n",
    "print(c)\n",
    "\n",
    "\n",
    "d = a.prod(dim=1)\n",
    "e = torch.cumprod(a, dim=1)\n",
    "print('\\n维度0上求积 与 累积')\n",
    "print(d)\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2）mean、median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([[2., 2., 5.],\n",
    "                  [3., 3., 8.],\n",
    "                  [4., 4., 4.]])\n",
    "\n",
    "print('求均值')\n",
    "print(torch.mean(a))\n",
    "print(torch.mean(a, 0))\n",
    "print(torch.mean(a, 1))\n",
    "\n",
    "print('\\n求中位数')\n",
    "print(torch.median(a))\n",
    "print(torch.median(a, 0))\n",
    "print(torch.median(a, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3）std、var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 创建示例张量\n",
    "a = torch.tensor([[1.0, 2.0],\n",
    "                  [3.0, 4.0]])\n",
    "\n",
    "# 计算张量的标准差\n",
    "std_value = torch.std(a)\n",
    "print(\"Standard deviation:\", std_value)\n",
    "\n",
    "# 计算张量的方差\n",
    "var_value = torch.var(a)\n",
    "print(\"Variance:\", var_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4）norm、dist\n",
    "torch.norm(t, p) ：t 的 p阶范数\n",
    "\n",
    "torch.dist(a, b, p) ：a，b 之间的 p阶范数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 创建示例张量\n",
    "tensor1 = torch.tensor([1.0, 2.0, 3.0])\n",
    "tensor2 = torch.tensor([4.0, 5.0, 6.0])\n",
    "\n",
    "# 使用 torch.norm() 计算张量的范数\n",
    "norm_value = torch.norm(tensor1)\n",
    "print(\"tensor1 的L2范数:\", norm_value)\n",
    "\n",
    "# 使用 torch.dist() 计算两个张量的范数\n",
    "dist_value = torch.dist(tensor1, tensor2)\n",
    "print(\"tensor1 和 tensor2 之间的L2范数:\", dist_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7、比较操作\n",
    "### 1）eq、 equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 示例张量\n",
    "t = torch.tensor([[1, 2, 3],\n",
    "                  [4, 5, 6]])\n",
    "\n",
    "# 使用eq()比较张量中的元素是否等于2\n",
    "result_eq = torch.eq(t, 2)\n",
    "print(\"Result of eq():\\n\", result_eq)\n",
    "\n",
    "\n",
    "# 使用equal()比较两个张量是否相等\n",
    "t1 = torch.tensor([[1, 2, 3],\n",
    "                   [4, 5, 6]])\n",
    "t2 = torch.tensor([[1, 2, 3],\n",
    "                   [4, 5, 6]])\n",
    "result_equal = torch.equal(t1, t2)\n",
    "print(\"\\nResult of equal():\", result_equal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2）gt、 lt、ge、 le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 示例张量\n",
    "t = torch.tensor([[1, 2, 3],\n",
    "                  [4, 5, 6]])\n",
    "\n",
    "# 使用gt()比较张量中的元素是否大于3\n",
    "result_gt = torch.gt(t, 3)\n",
    "print(\"Result of gt():\\n\", result_gt)\n",
    "\n",
    "\n",
    "# 使用lt()比较张量中的元素是否小于3\n",
    "result_lt = torch.lt(t, 3)\n",
    "print(\"\\nResult of lt():\\n\", result_lt)\n",
    "\n",
    "# 使用gt()比较张量中的元素是否大于等于3\n",
    "result_ge = torch.ge(t, 3)\n",
    "print(\"\\nResult of ge():\\n\", result_ge)\n",
    "\n",
    "\n",
    "# 使用lt()比较张量中的元素是否小于等于3\n",
    "result_le = torch.le(t, 3)\n",
    "print(\"\\nResult of le():\\n\", result_le)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3）max、min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 示例张量\n",
    "t = torch.tensor([[1, 2, 3],\n",
    "                  [4, 5, 6]])\n",
    "\n",
    "# 计算张量的最大值和最小值\n",
    "max_value = torch.max(t)\n",
    "min_value = torch.min(t)\n",
    "print(\"Max value:\", max_value)\n",
    "print(\"Min value:\", min_value)\n",
    "\n",
    "# 沿着指定维度计算张量的最大值和最小值\n",
    "max_value_axis_0 = torch.max(t, axis=0)\n",
    "min_value_axis_1 = torch.min(t, axis=1)\n",
    "print(\"\\n沿0维上的最大值:\", max_value_axis_0.values)\n",
    "print(\"沿0维上的最大值索引:\", max_value_axis_0.indices)\n",
    "print(\"沿1维上的最小值:\", min_value_axis_1.values)\n",
    "print(\"沿1维上的最小值索引:\", min_value_axis_1.indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4）topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 示例张量\n",
    "t = torch.tensor([[1, 2, 3],\n",
    "                  [4, 5, 6]])\n",
    "\n",
    "# 沿着指定维度获取张量中最大的两个值及其索引\n",
    "topk_values, topk_indices = torch.topk(t, k=2, dim=1)\n",
    "print(\"沿维度1上的最大的2个值:\\n\", topk_values)\n",
    "print(\"\\n沿维度1上的最大的2个值的索引:\\n\", topk_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8、矩阵操作\n",
    "### 1）dot\n",
    "Torch的 dot 只能对两个 一维张量 进行点积运算，否则会报错；Numpy中的dot无此限制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([2, 3])\n",
    "b = torch.tensor([3, 4])\n",
    "\n",
    "print(torch.dot(a, b))\n",
    "\n",
    "import torch\n",
    "\n",
    "a = torch.tensor([[2, 3],\n",
    "                  [3, 4]])\n",
    "\n",
    "b = torch.tensor([[3, 4],\n",
    "                  [1, 2]])\n",
    "\n",
    "# print(torch.dot(a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2）mul\n",
    "a 和 b 必须尺寸相同。\n",
    "\n",
    "torch.mul(a, b) 和 a * b 效果一样\n",
    "\n",
    "torch.mul(a, b) 是逐元素相乘，torch.mm(a, b) 是矩阵相乘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([[2, 3],\n",
    "                  [3, 4]])\n",
    "\n",
    "b = torch.tensor([[3, 4],\n",
    "                  [1, 2]])\n",
    "\n",
    "print(torch.mul(a, b))\n",
    "print(a * b)\n",
    "print(torch.mm(a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3）mm、mv\n",
    "torch.mm(t1, t2) 是矩阵相乘， torch.mv(t, v) 矩阵与向量乘法\n",
    "\n",
    "torch.mv(t, v) , 矩阵t为第一个参数，向量v为第二个参数，位置不能换，否则会报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([[1, 2, 3],\n",
    "                 [2, 3, 4]])\n",
    "\n",
    "b = torch.tensor([[1, 2],\n",
    "                  [1, 2],\n",
    "                  [3, 4]])\n",
    "\n",
    "c = torch.tensor([1, 2, 3])\n",
    "\n",
    "print(torch.mm(a, b))\n",
    "print(torch.mv(a, c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4）bmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "batch1 = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]], dtype=torch.float32)  # Shape: (2, 2, 2)\n",
    "batch2 = torch.tensor([[[2, 0], [0, 2]], [[1, 0], [0, 1]]], dtype=torch.float32)  # Shape: (2, 2, 2)\n",
    "\n",
    "result = torch.bmm(batch1, batch2)\n",
    "print(\"Result:\\n\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5）svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.randn(2, 3)\n",
    "print(torch.svd(a))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
